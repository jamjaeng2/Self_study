{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "julian-button",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cuda\n",
      "============================================\n",
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.306480\n",
      "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.306780\n",
      "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.313607\n",
      "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.315940\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.309420\n",
      "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.297213\n",
      "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.313688\n",
      "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.309401\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.298493\n",
      "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.306395\n",
      "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.300863\n",
      "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.306460\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.303512\n",
      "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.296058\n",
      "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.296024\n",
      "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.303452\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.304398\n",
      "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.300750\n",
      "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.299310\n",
      "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.280153\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.308274\n",
      "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.299073\n",
      "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.292531\n",
      "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.291113\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.284796\n",
      "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.292814\n",
      "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.293875\n",
      "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.292329\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.298186\n",
      "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.292710\n",
      "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.296983\n",
      "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.287217\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.266208\n",
      "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.281567\n",
      "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.285650\n",
      "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.286252\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.291901\n",
      "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.284182\n",
      "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.278310\n",
      "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.276723\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.291971\n",
      "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.287449\n",
      "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.286980\n",
      "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.261708\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.284759\n",
      "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.275946\n",
      "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.284735\n",
      "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.269709\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.276597\n",
      "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.278681\n",
      "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.264371\n",
      "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.257729\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.255577\n",
      "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.255108\n",
      "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.249468\n",
      "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.257129\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.255539\n",
      "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.246587\n",
      "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.249346\n",
      "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.249348\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.227501\n",
      "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.236780\n",
      "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.217410\n",
      "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.206318\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.209675\n",
      "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.224832\n",
      "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.197328\n",
      "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.181873\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.184345\n",
      "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.153990\n",
      "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.114100\n",
      "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.124780\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.155825\n",
      "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.100751\n",
      "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.076960\n",
      "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.061730\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 1.984838\n",
      "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 1.927873\n",
      "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 1.962962\n",
      "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 1.832016\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 1.906203\n",
      "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 1.883131\n",
      "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 1.811657\n",
      "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 1.871273\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.719625\n",
      "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.667703\n",
      "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.578910\n",
      "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.653235\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.500775\n",
      "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.476101\n",
      "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.550415\n",
      "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.514117\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.457956\n",
      "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.288402\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0211, Accuracy: 5579/10000 (56%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.498879\n",
      "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.244604\n",
      "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.294559\n",
      "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.345841\n",
      "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.222258\n",
      "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.032871\n",
      "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 0.994122\n",
      "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 1.122366\n",
      "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 1.167107\n",
      "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.857453\n",
      "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.797244\n",
      "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 0.868355\n",
      "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.841712\n",
      "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.818014\n",
      "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 1.062012\n",
      "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 1.164196\n",
      "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.979740\n",
      "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.728132\n",
      "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.716760\n",
      "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 1.018056\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.708946\n",
      "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.956569\n",
      "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.918853\n",
      "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.745525\n",
      "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.670196\n",
      "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.836525\n",
      "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.742546\n",
      "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.582123\n",
      "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.590037\n",
      "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.596094\n",
      "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.578181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.811085\n",
      "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.758354\n",
      "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.557521\n",
      "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.834337\n",
      "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.613252\n",
      "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.677008\n",
      "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.842651\n",
      "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.488547\n",
      "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.787176\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.568049\n",
      "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.554515\n",
      "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.615098\n",
      "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.593137\n",
      "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.524372\n",
      "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.564697\n",
      "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.662072\n",
      "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.423994\n",
      "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.554598\n",
      "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.577946\n",
      "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.510515\n",
      "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.571793\n",
      "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.502979\n",
      "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.564519\n",
      "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.440598\n",
      "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.813185\n",
      "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.399014\n",
      "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.583997\n",
      "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.414144\n",
      "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.492282\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.338017\n",
      "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.403882\n",
      "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.642377\n",
      "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.481651\n",
      "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.863306\n",
      "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.587363\n",
      "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.553676\n",
      "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.421524\n",
      "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.452081\n",
      "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.418039\n",
      "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.543247\n",
      "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.480904\n",
      "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.499437\n",
      "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.424558\n",
      "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.481143\n",
      "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.489471\n",
      "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.719223\n",
      "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.367686\n",
      "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.312753\n",
      "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.538734\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.635040\n",
      "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.501510\n",
      "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.333590\n",
      "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.588422\n",
      "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.357059\n",
      "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.268912\n",
      "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.446416\n",
      "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.370554\n",
      "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.401330\n",
      "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.452438\n",
      "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.369745\n",
      "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.486893\n",
      "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.742359\n",
      "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.226378\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0062, Accuracy: 8842/10000 (88%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.313944\n",
      "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.348915\n",
      "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.293103\n",
      "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.563882\n",
      "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.514751\n",
      "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.603126\n",
      "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.417620\n",
      "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.419328\n",
      "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.398962\n",
      "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.240690\n",
      "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.597232\n",
      "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.323389\n",
      "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.376433\n",
      "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.454031\n",
      "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.371468\n",
      "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.208655\n",
      "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.358098\n",
      "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.521960\n",
      "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.565488\n",
      "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.317261\n",
      "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.287440\n",
      "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.309786\n",
      "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.568149\n",
      "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.394834\n",
      "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.249721\n",
      "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.304862\n",
      "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.492462\n",
      "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.319766\n",
      "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.286982\n",
      "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.420492\n",
      "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.401621\n",
      "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.266294\n",
      "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.339509\n",
      "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.368310\n",
      "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.229381\n",
      "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.335757\n",
      "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.384846\n",
      "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.383665\n",
      "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.336266\n",
      "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.300090\n",
      "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.321708\n",
      "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.468443\n",
      "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.250412\n",
      "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.219511\n",
      "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.160283\n",
      "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.539734\n",
      "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.433114\n",
      "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.298945\n",
      "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.236057\n",
      "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.371731\n",
      "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.090531\n",
      "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.297466\n",
      "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.331709\n",
      "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.203960\n",
      "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.161713\n",
      "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.444375\n",
      "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.275490\n",
      "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.294617\n",
      "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.215595\n",
      "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.264673\n",
      "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.352856\n",
      "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.222471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.364996\n",
      "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.497003\n",
      "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.512349\n",
      "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.330524\n",
      "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.417478\n",
      "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.227957\n",
      "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.167329\n",
      "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.472343\n",
      "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.185144\n",
      "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.423554\n",
      "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.213738\n",
      "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.389031\n",
      "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.242079\n",
      "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.127408\n",
      "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.194095\n",
      "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.243516\n",
      "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.390261\n",
      "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.533388\n",
      "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.418322\n",
      "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.244595\n",
      "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.279550\n",
      "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.192315\n",
      "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.311346\n",
      "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.361921\n",
      "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.266436\n",
      "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.267073\n",
      "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.162698\n",
      "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.286108\n",
      "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.589606\n",
      "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.321237\n",
      "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.304576\n",
      "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.403831\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0047, Accuracy: 9099/10000 (91%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.388839\n",
      "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.228858\n",
      "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.316385\n",
      "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.200664\n",
      "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.275738\n",
      "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.259268\n",
      "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.343042\n",
      "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.245388\n",
      "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.410492\n",
      "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.282843\n",
      "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.307556\n",
      "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.177711\n",
      "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.249279\n",
      "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.333621\n",
      "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.191732\n",
      "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.310278\n",
      "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.339830\n",
      "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.264632\n",
      "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.211016\n",
      "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.332611\n",
      "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.288802\n",
      "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.309960\n",
      "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.594449\n",
      "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.292236\n",
      "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.245606\n",
      "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.212144\n",
      "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.134355\n",
      "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.362470\n",
      "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.187421\n",
      "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.189689\n",
      "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.204134\n",
      "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.221917\n",
      "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.131444\n",
      "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.380261\n",
      "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.304906\n",
      "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.150556\n",
      "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.293233\n",
      "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.279858\n",
      "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.614840\n",
      "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.313653\n",
      "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.131166\n",
      "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.254979\n",
      "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.203197\n",
      "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.224248\n",
      "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.121980\n",
      "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.357516\n",
      "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.175518\n",
      "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.278365\n",
      "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.213696\n",
      "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.411856\n",
      "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.284750\n",
      "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.272619\n",
      "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.282922\n",
      "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.240506\n",
      "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.243447\n",
      "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.242431\n",
      "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.233177\n",
      "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.208187\n",
      "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.153589\n",
      "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.396958\n",
      "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.223283\n",
      "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.247410\n",
      "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.148481\n",
      "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.257875\n",
      "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.267441\n",
      "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.319353\n",
      "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.317022\n",
      "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.216514\n",
      "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.308614\n",
      "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.280226\n",
      "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.230174\n",
      "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.281621\n",
      "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.096784\n",
      "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.181711\n",
      "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.229354\n",
      "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.278053\n",
      "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.159347\n",
      "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.082088\n",
      "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.161320\n",
      "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.250002\n",
      "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.248996\n",
      "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.148950\n",
      "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.122748\n",
      "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.213262\n",
      "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.288806\n",
      "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.271170\n",
      "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.126581\n",
      "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.218131\n",
      "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.186428\n",
      "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.239337\n",
      "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.245189\n",
      "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.177870\n",
      "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.202563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.205376\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0033, Accuracy: 9377/10000 (94%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.205681\n",
      "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.092706\n",
      "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.092938\n",
      "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.191659\n",
      "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.256281\n",
      "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.117471\n",
      "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.118281\n",
      "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.134575\n",
      "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.270285\n",
      "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.156847\n",
      "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.079984\n",
      "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.233828\n",
      "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.373692\n",
      "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.354338\n",
      "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.109325\n",
      "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.158024\n",
      "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.247943\n",
      "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.296633\n",
      "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.204314\n",
      "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.272438\n",
      "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.372886\n",
      "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.278001\n",
      "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.237765\n",
      "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.364727\n",
      "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.499484\n",
      "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.149803\n",
      "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.417176\n",
      "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.153417\n",
      "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.197418\n",
      "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.083735\n",
      "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.293479\n",
      "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.237864\n",
      "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.279252\n",
      "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.116298\n",
      "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.301730\n",
      "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.248817\n",
      "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.303389\n",
      "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.166978\n",
      "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.038957\n",
      "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.363482\n",
      "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.174929\n",
      "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.171541\n",
      "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.155922\n",
      "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.218093\n",
      "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.243488\n",
      "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.193684\n",
      "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.100249\n",
      "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.177693\n",
      "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.145984\n",
      "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.171696\n",
      "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.237149\n",
      "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.160974\n",
      "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.174015\n",
      "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.132393\n",
      "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.125551\n",
      "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.166476\n",
      "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.128786\n",
      "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.113664\n",
      "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.350343\n",
      "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.363586\n",
      "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.158884\n",
      "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.178917\n",
      "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.236646\n",
      "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.163028\n",
      "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.179546\n",
      "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.165831\n",
      "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.084318\n",
      "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.098208\n",
      "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.168218\n",
      "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.396836\n",
      "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.469351\n",
      "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.126937\n",
      "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.294050\n",
      "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.114102\n",
      "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.198599\n",
      "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.247743\n",
      "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.110474\n",
      "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.205942\n",
      "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.060340\n",
      "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.241799\n",
      "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.144617\n",
      "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.175452\n",
      "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.261867\n",
      "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.195304\n",
      "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.227159\n",
      "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.088144\n",
      "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.037830\n",
      "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.055167\n",
      "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.188465\n",
      "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.341825\n",
      "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.130337\n",
      "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.041600\n",
      "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.242302\n",
      "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.270252\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0027, Accuracy: 9506/10000 (95%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.293717\n",
      "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.404568\n",
      "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.230886\n",
      "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.138611\n",
      "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.066393\n",
      "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.197367\n",
      "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.079531\n",
      "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.295743\n",
      "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.263935\n",
      "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.144640\n",
      "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.068170\n",
      "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.117256\n",
      "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.185465\n",
      "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.118429\n",
      "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.171194\n",
      "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.061202\n",
      "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.225611\n",
      "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.062820\n",
      "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.164156\n",
      "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.057154\n",
      "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.151043\n",
      "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.132838\n",
      "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.272514\n",
      "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.079689\n",
      "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.290968\n",
      "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.059868\n",
      "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.132573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.115253\n",
      "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.136746\n",
      "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.078925\n",
      "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.160594\n",
      "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.112361\n",
      "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.255122\n",
      "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.274306\n",
      "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.200836\n",
      "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.198156\n",
      "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.066168\n",
      "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.137651\n",
      "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.389973\n",
      "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.077039\n",
      "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.052341\n",
      "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.185583\n",
      "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.228732\n",
      "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.058037\n",
      "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.318600\n",
      "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.253995\n",
      "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.104556\n",
      "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.187162\n",
      "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.166831\n",
      "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.180234\n",
      "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.112085\n",
      "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.165639\n",
      "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.200899\n",
      "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.088695\n",
      "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.402882\n",
      "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.311348\n",
      "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.115844\n",
      "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.179514\n",
      "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.223990\n",
      "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.099996\n",
      "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.082246\n",
      "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.130966\n",
      "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.102461\n",
      "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.146915\n",
      "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.247980\n",
      "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.297878\n",
      "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.074204\n",
      "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.152598\n",
      "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.139514\n",
      "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.236200\n",
      "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.150988\n",
      "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.082457\n",
      "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.110838\n",
      "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.093608\n",
      "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.050980\n",
      "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.258273\n",
      "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.231762\n",
      "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.249654\n",
      "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.125872\n",
      "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.159034\n",
      "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.180001\n",
      "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.130866\n",
      "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.135899\n",
      "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.111187\n",
      "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.123557\n",
      "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.274139\n",
      "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.068114\n",
      "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.097534\n",
      "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.078183\n",
      "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.062860\n",
      "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.167411\n",
      "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.050537\n",
      "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.131079\n",
      "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.190184\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0027, Accuracy: 9466/10000 (95%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.152273\n",
      "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.143713\n",
      "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.125435\n",
      "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.073593\n",
      "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.045807\n",
      "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.044878\n",
      "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.105522\n",
      "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.157091\n",
      "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.080676\n",
      "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.055776\n",
      "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.138852\n",
      "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.117531\n",
      "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.149347\n",
      "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.077307\n",
      "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.128617\n",
      "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.104022\n",
      "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.140169\n",
      "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.056040\n",
      "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.218871\n",
      "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.089733\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.159652\n",
      "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.081075\n",
      "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.110176\n",
      "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.078535\n",
      "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.067137\n",
      "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.061118\n",
      "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.183384\n",
      "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.113109\n",
      "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.126588\n",
      "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.139152\n",
      "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.168550\n",
      "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.239442\n",
      "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.160007\n",
      "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.204357\n",
      "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.138983\n",
      "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.119400\n",
      "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.019608\n",
      "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.152725\n",
      "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.118645\n",
      "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.065366\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.186205\n",
      "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.156758\n",
      "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.207680\n",
      "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.085882\n",
      "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.136650\n",
      "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.128177\n",
      "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.279666\n",
      "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.123626\n",
      "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.076886\n",
      "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.049636\n",
      "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.109204\n",
      "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.125971\n",
      "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.093138\n",
      "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.066004\n",
      "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.097767\n",
      "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.050744\n",
      "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.148540\n",
      "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.145963\n",
      "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.147747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.102627\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.138782\n",
      "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.079404\n",
      "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.174428\n",
      "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.123730\n",
      "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.049294\n",
      "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.208066\n",
      "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.069624\n",
      "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.101927\n",
      "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.082161\n",
      "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.279785\n",
      "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.153978\n",
      "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.033465\n",
      "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.194877\n",
      "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.105264\n",
      "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.076875\n",
      "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.313293\n",
      "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.135496\n",
      "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.161811\n",
      "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.090567\n",
      "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.068180\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.181454\n",
      "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.184228\n",
      "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.075421\n",
      "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.108173\n",
      "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.105670\n",
      "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.221322\n",
      "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.077721\n",
      "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.104358\n",
      "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.074060\n",
      "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.145883\n",
      "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.118880\n",
      "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.166748\n",
      "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.048309\n",
      "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.149918\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0019, Accuracy: 9631/10000 (96%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.097623\n",
      "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.120754\n",
      "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.186985\n",
      "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.135527\n",
      "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.191256\n",
      "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.098586\n",
      "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.083753\n",
      "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.149404\n",
      "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.234225\n",
      "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.181452\n",
      "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.096070\n",
      "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.037037\n",
      "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.099103\n",
      "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.096797\n",
      "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.080409\n",
      "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.088063\n",
      "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.295130\n",
      "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.195051\n",
      "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.143509\n",
      "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.089973\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.170565\n",
      "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.155558\n",
      "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.081264\n",
      "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.055900\n",
      "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.120662\n",
      "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.178733\n",
      "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.163450\n",
      "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.053269\n",
      "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.154787\n",
      "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.098382\n",
      "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.139614\n",
      "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.157208\n",
      "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.149429\n",
      "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.077108\n",
      "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.135462\n",
      "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.128154\n",
      "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.052839\n",
      "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.085369\n",
      "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.061174\n",
      "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.267426\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.237389\n",
      "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.037752\n",
      "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.114674\n",
      "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.047100\n",
      "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.074021\n",
      "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.213688\n",
      "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.085746\n",
      "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.065425\n",
      "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.124161\n",
      "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.061244\n",
      "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.046365\n",
      "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.121908\n",
      "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.026916\n",
      "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.129656\n",
      "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.253401\n",
      "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.075448\n",
      "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.087061\n",
      "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.098661\n",
      "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.126846\n",
      "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.198565\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.161560\n",
      "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.153733\n",
      "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.186137\n",
      "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.038240\n",
      "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.184038\n",
      "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.072842\n",
      "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.086762\n",
      "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.055022\n",
      "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.037986\n",
      "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.209041\n",
      "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.024416\n",
      "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.105264\n",
      "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.033040\n",
      "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.112541\n",
      "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.096517\n",
      "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.066444\n",
      "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.078997\n",
      "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.037057\n",
      "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.170484\n",
      "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.031394\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.090316\n",
      "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.090321\n",
      "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.086822\n",
      "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.057941\n",
      "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.094912\n",
      "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.096648\n",
      "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.085096\n",
      "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.045968\n",
      "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.110418\n",
      "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.064919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.063678\n",
      "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.020826\n",
      "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.041298\n",
      "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.055037\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0019, Accuracy: 9643/10000 (96%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.081652\n",
      "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.044688\n",
      "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.055245\n",
      "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.114033\n",
      "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.039245\n",
      "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.155027\n",
      "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.044279\n",
      "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.026395\n",
      "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.023603\n",
      "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.100200\n",
      "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.047903\n",
      "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.190902\n",
      "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.030545\n",
      "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.095074\n",
      "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.151739\n",
      "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.089257\n",
      "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.036424\n",
      "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.136162\n",
      "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.090847\n",
      "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.022759\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.108205\n",
      "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.078428\n",
      "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.190124\n",
      "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.037181\n",
      "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.117924\n",
      "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.240457\n",
      "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.144514\n",
      "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.053114\n",
      "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.018067\n",
      "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.043458\n",
      "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.089574\n",
      "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.147932\n",
      "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.062789\n",
      "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.055176\n",
      "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.070007\n",
      "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.045297\n",
      "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.016487\n",
      "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.088318\n",
      "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.129710\n",
      "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.155318\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.099533\n",
      "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.033618\n",
      "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.027540\n",
      "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.111548\n",
      "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.047704\n",
      "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.042870\n",
      "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.091509\n",
      "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.109820\n",
      "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.062784\n",
      "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.080427\n",
      "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.139946\n",
      "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.078487\n",
      "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.142652\n",
      "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.043418\n",
      "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.035778\n",
      "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.096833\n",
      "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.077179\n",
      "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.025844\n",
      "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.053900\n",
      "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.098002\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.078948\n",
      "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.123499\n",
      "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.205755\n",
      "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.135555\n",
      "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.131620\n",
      "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.107661\n",
      "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.100726\n",
      "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.074944\n",
      "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.084101\n",
      "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.059675\n",
      "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.045721\n",
      "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.063344\n",
      "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.052575\n",
      "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.104896\n",
      "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.062406\n",
      "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.171986\n",
      "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.156390\n",
      "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.108162\n",
      "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.045933\n",
      "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.034564\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.034735\n",
      "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.144992\n",
      "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.194287\n",
      "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.042520\n",
      "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.094374\n",
      "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.043050\n",
      "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.034175\n",
      "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.190755\n",
      "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.077365\n",
      "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.097070\n",
      "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.036197\n",
      "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.026629\n",
      "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.033559\n",
      "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.061330\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0018, Accuracy: 9660/10000 (97%)\n",
      "Testing time: 0m 7s\n",
      "Total Time: 1m 2s\n",
      "Model was trained on cuda!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-cinema",
   "metadata": {},
   "source": [
    "Transforms.Totensor() : 데이터 타입을 Tensor 형태로 변경. <br>\n",
    "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) : 이미지 픽셀은 0~255 값을 가지나, ToTensor 변경시 0 ~ 1로 바뀜.<br>\n",
    "Normalize로 -1 ~ 1 사이로 정규화.<br>\n",
    "ToTensor() : scaling을 해준거고, Normalize를 해주면 centering + rescaling 해준것. 정확하게 하기 위해 <br>\n",
    "학습 데이터 별 픽셀별로 평균을 구하던가 채널별로 평균을 구해서 centering 해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-positive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-leisure",
   "metadata": {},
   "source": [
    "Conv2d : torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, gropus = 1, bias = True <br>\n",
    "첫번째 채널과 ㄷ구번째 채널의 수를 같게 해줘야 함.((1,10) -> (10,20)) 10으로 같게 <br>\n",
    "MaxPoold2d : torch.nn.MaxPool2d(kernel_size, stride = None, Padding = 0, dilation = 1, return_indices=False, ceil_mode = False) <br>\n",
    "Linear : torch.nn.Linear(in_features, out_features, bias =True) <br>\n",
    "\n",
    "in_features 를 320으로 한 이유 : 아무숫자 100을 넣고 생기는 에러를 보고 조절 함.\n",
    "m1에 있는 320과 m2에 있는 100이 같지 않아서 생기는 에러로.. 수정 확인 <br>\n",
    "\n",
    "forward 함수를 넣으면 convolution 거치고, maxpooling 거치고, relu를 거치고 한번 더 반복 후 Flatten 후 FC 레이어 통과."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interested-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "        \n",
    "    def forwrad(self, x):\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1) # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = Net()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forwrad(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "    \n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-grammar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attractive-liberia",
   "metadata": {},
   "source": [
    "optimizer : SGD\n",
    "optimizer.zero_grad()는 옵티마이져의 그래디언트를 0으로 초기화 해주는 작업.\n",
    "nll_loss를 이용해서 loss를 계산.\n",
    "loss.backward()로 back propagation 하고, optimizer.step()을 이용해서 업데이트 해주면서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "romance-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(taregt)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{} / {} ({:.0f}%)]\\tLoss : {:.6f}'.forat(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-debate",
   "metadata": {},
   "source": [
    "테스트 로스와 정답 개수(correct)를 0으로 초기화. <br>\n",
    "testset의 img(data)와 label(target)을 불러옴 <br>\n",
    "test_loss += F.nll_loss(output, target).data[0]으로 배치 loss의 합을 구함 <br>\n",
    "이때 data[0]은 텐서로 되어있는 로스를 보기위한 함수 <br>\n",
    "\n",
    "pred = output.data.max(1, keepdim=True)[1] 부분에서 max 함수 안의 1은 어느 방향으로 max값을 찾을지를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prescription-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile = True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, tartget).data[0]\n",
    "        #get the indeex of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\n Test set: Average loss : {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.*correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cordless-journal",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-51afd2885487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-eeccc47af534>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaregt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "moved-force",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-25f3036a1cea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training time : {m:.0f}m {s:.0f}s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-eeccc47af534>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaregt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time : {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time,time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "        \n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m, {s:.0f}s\\nMoel was trained on {device}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-charleston",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
